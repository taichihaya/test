{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2_BERT_livedoor_news_on_Google_Colaboratory_label_smoothing のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taichihaya/test/blob/master/2_BERT_livedoor_news_add_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYlVm0kpjx57"
      },
      "source": [
        "## 日本語BERTでlivedoorニュースを教師あり学習で分類"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIW6ar2Bm3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6788583-228f-4aad-ae75-cd4256b09307"
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import tarfile\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fea1e6cdb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6dbVUfj1W-"
      },
      "source": [
        "## 準備1：Livedoorニュースをダウンロードしてtsvファイル化\n",
        "\n",
        "参考：https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keY2WGdwjzLD"
      },
      "source": [
        "%%capture\n",
        "# Livedoorニュースのファイルをダウンロード\n",
        "! wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
        "\n",
        "# 解凍\n",
        "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
        "tar.extractall(\"./data/livedoor/\")\n",
        "tar.close()\n",
        "\n",
        "# ファイルの中身を確認してみる\n",
        "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
        "\n",
        "# 本文を取得する前処理関数を定義\n",
        "\n",
        "def extract_main_txt(file_name):\n",
        "    with open(file_name) as text_file:\n",
        "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
        "        text = text_file.readlines()[3:]\n",
        "\n",
        "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
        "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
        "        text = list(filter(lambda line: line != '', text))\n",
        "        text = ''.join(text)\n",
        "        text = text.translate(str.maketrans(\n",
        "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
        "        return text\n",
        "\n",
        "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
        "\n",
        "list_text = []\n",
        "list_label = []\n",
        "\n",
        "for cat in categories:\n",
        "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
        "\n",
        "    # 前処理extract_main_txtを実施して本文を取得\n",
        "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
        "\n",
        "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
        "\n",
        "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
        "    list_label.extend(label)\n",
        "\n",
        "# pandasのDataFrameにする\n",
        "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
        "\n",
        "# カテゴリーの辞書を作成\n",
        "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
        "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
        "\n",
        "# DataFrameにカテゴリーindexの列を作成\n",
        "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
        "\n",
        "# label列を消去し、text, indexの順番にする\n",
        "df = df.loc[:, [\"text\", \"label_index\"]]\n",
        "\n",
        "# 順番をシャッフルする\n",
        "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "# 適当に説明変数作成\n",
        "df[\"var1\"] = df.text.str.contains(\"1\")*1\n",
        "df[\"var2\"] = df.text.str.contains(\"2\")*1\n",
        "df[\"var3\"] = df.text.str.contains(\"3\")*1\n",
        "\n",
        "# tsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "test = df[:len_0_2]\n",
        "test.to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "train = df[len_0_2:]\n",
        "train.to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPT3Pjr94oPW"
      },
      "source": [
        "## 準備2：LivedoorニュースをBERT用のDataLoaderにする\n",
        "\n",
        "Hugginfaceのリポジトリの案内とは異なり、torchtextを使用した手法で実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXX4pr2-kY-"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "%%capture\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7\n",
        "!pip install transformers==2.9.0\n",
        "\n",
        "import torch\n",
        "import torchtext  # torchtextを使用\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "\n",
        "# 日本語BERTの分かち書き用tokenizerです\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "    'bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMnVMn_CYjgh"
      },
      "source": [
        "1列目：テキスト  \n",
        "2列目：ラベル  \n",
        "3列目以降：説明変数  \n",
        "となるようなtsvを用意する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "nJzXC-1TYz47",
        "outputId": "663f6678-53ea-4c80-d0b6-d368a368c5b6"
      },
      "source": [
        "train_eval = pd.read_csv(\"./train_eval.tsv\",sep=\"\\t\",header=None)\n",
        "test = pd.read_csv(\"./test.tsv\",sep=\"\\t\",header=None)\n",
        "train_eval.head(2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ジャーナリストの田原総一朗が、「非常に面白い番組になると思います」と自信たっぷりのコメント。...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ゼロから始めるスマートフォンスクウェア・エニックス（スクエニ）は27日、人気のロールプレイン...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  1  2  3  4\n",
              "0  ジャーナリストの田原総一朗が、「非常に面白い番組になると思います」と自信たっぷりのコメント。...  3  1  1  1\n",
              "1  ゼロから始めるスマートフォンスクウェア・エニックス（スクエニ）は27日、人気のロールプレイン...  4  1  1  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML7Ul0wVaEV8"
      },
      "source": [
        "torchtextを使用して文章をID化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZypBWaE-PB6"
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "\n",
        "    return  tokenizer.encode_plus(\n",
        "                                input_text,                      \n",
        "                                add_special_tokens = True, # Special Tokenの追加\n",
        "                                max_length = 512,           # 文章の長さを固定（Padding/Trancatinating）\n",
        "                                pad_to_max_length = True,# PADDINGで埋める\n",
        "                                return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
        "                          )[\"input_ids\"]\n",
        "\n",
        "    # return tokenizer.encode(input_text, max_length=512, return_tensors='pt')\n",
        "\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
        "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# (注釈)：各引数を再確認\n",
        "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
        "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
        "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
        "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
        "# include_length: 文章の単語数のデータを保持するか\n",
        "# batch_first：ミニバッチの次元を用意するかどうか\n",
        "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
        "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyLNL-sd_Xd5"
      },
      "source": [
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
        "# 少し時間がかかります\n",
        "# train_eval：5901個、test：1475個\n",
        "# dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(\n",
        "#     path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT)])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzid3x6CanfY"
      },
      "source": [
        "ID化されたtextと目的変数、説明変数をtensorに変換　→ TensorDatasetに格納"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbc1-JMHiXwT",
        "outputId": "56a325a1-679f-46ef-e431-71ccc570aff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = df\n",
        "test.var1 = test.var1.astype(\"int64\")\n",
        "test.var2 = test.var2.astype(\"int64\")\n",
        "test.var3 = test.var3.astype(\"int64\")\n",
        "test.text = test.text.apply(tokenizer_512)\n",
        "test.dtypes"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text           object\n",
              "label_index     int64\n",
              "var1            int64\n",
              "var2            int64\n",
              "var3            int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJtaB23qhh5r"
      },
      "source": [
        "ids = torch.cat(list(np.array(df.text)),dim=0)\n",
        "label_and_vars = torch.tensor(np.array(test.iloc[:,1:]))\n",
        "tensor_dataset = TensorDataset(ids,label_and_vars)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKB4Bzm0hht-",
        "outputId": "a71be45a-0fe0-444d-a488-314714aec603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ids.shape,label_and_vars.shape"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7376, 512]), torch.Size([7376, 4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhgsFRCTG6KD"
      },
      "source": [
        "# trainとvalidに分ける\n",
        "train_size = int(0.8 * len(tensor_dataset))\n",
        "val_size = len(tensor_dataset) - train_size\n",
        "\n",
        "# データセットを分割\n",
        "train_eval_dataset, test_dataset = random_split(tensor_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(train_eval_dataset))\n",
        "val_size = len(train_eval_dataset) - train_size\n",
        "\n",
        "train_dataset, eval_dataset = random_split(train_eval_dataset, [train_size, val_size])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxaGFyXGblHx"
      },
      "source": [
        "DataLoaderにまとめる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz_FnpJzXSer"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# 訓練データローダー\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化\n",
        "            batch_size = 16\n",
        "        )\n",
        "\n",
        "# 検証データローダー\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
        "            batch_size = 16\n",
        "        )"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxyTfR0XswI"
      },
      "source": [
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\":validation_dataloader }"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8e6NhQ3cLcq"
      },
      "source": [
        "## 準備3：BERTのクラス分類用のモデルを用意する\n",
        "\n",
        "Huggingfaceさんのをそのまま使うのではなく、BERTのbaseだけ使い、残りは自分で実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFlvnI05a4xN"
      },
      "source": [
        "from transformers.modeling_bert import BertModel\n",
        "\n",
        "# BERTの日本語学習済みパラメータのモデルです\n",
        "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HURTkPdrhglR"
      },
      "source": [
        "num_vars = len(train.columns) - 2"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BgGd7fLPssV"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class BertForLivedoor(nn.Module):\n",
        "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForLivedoor, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768 + 説明変数 、出力は9クラス\n",
        "        self.cls = nn.Linear(in_features=768+num_vars, out_features=9)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, vars):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "\n",
        "        # [batch_size, 説明変数の数]に変換された説明変数を追加\n",
        "        vec_0_and_vars = torch.cat([vec_0,vars],dim=1)\n",
        "        \n",
        "        output = self.cls(vec_0_and_vars)  # 全結合層\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOtveynWRwK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6e19a0-2481-429b-d48e-a3e1b0cf2c3c"
      },
      "source": [
        "# モデル構築\n",
        "net = BertForLivedoor()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg7r2RIR7qU"
      },
      "source": [
        "## 準備4：BERTのファインチューニングの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGYvX4RR3UT"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWYbBdkF5Uvp"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "    def forward(self, x, target, smoothing=0.1):\n",
        "        confidence = 1. - smoothing\n",
        "        logprobs = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = confidence * nll_loss + smoothing * smooth_loss\n",
        "        return loss.mean()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il_-wow4Suwe"
      },
      "source": [
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = LabelSmoothingCrossEntropy() "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7KRn1bTIQp"
      },
      "source": [
        "## 5. 訓練を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNaAXgiITFiw"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch[0].to(device)  # 文章\n",
        "                labels = batch[1].to(device)  # ラベル\n",
        "                vars = torch.cat(batch[2:],dim=0).view(-1,num_vars).to(device)  # 説明変数\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs,vars)\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfnH-gAmS75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a754066f-9f39-4095-be48-9ebc0dc8a887"
      },
      "source": [
        "# 学習・検証を実行する。1epochに2分ほどかかります\n",
        "import torch.nn.functional as F\n",
        "num_epochs = 4\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 2.1397 || 10iter. || 本イテレーションの正解率：0.0625\n",
            "イテレーション 20 || Loss: 2.0328 || 10iter. || 本イテレーションの正解率：0.3125\n",
            "イテレーション 30 || Loss: 1.8565 || 10iter. || 本イテレーションの正解率：0.5\n",
            "イテレーション 40 || Loss: 1.6472 || 10iter. || 本イテレーションの正解率：0.4375\n",
            "イテレーション 50 || Loss: 1.4278 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 60 || Loss: 1.2180 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 70 || Loss: 1.3441 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 80 || Loss: 1.3651 || 10iter. || 本イテレーションの正解率：0.5\n",
            "イテレーション 90 || Loss: 1.0898 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 100 || Loss: 0.8530 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 110 || Loss: 1.0088 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 120 || Loss: 0.9307 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 130 || Loss: 1.0997 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 140 || Loss: 1.1255 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 150 || Loss: 0.7572 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 160 || Loss: 0.9648 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 170 || Loss: 0.8398 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 180 || Loss: 1.0894 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 190 || Loss: 0.7708 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 1.1180 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 210 || Loss: 0.7383 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 220 || Loss: 1.0784 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 230 || Loss: 0.8127 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 240 || Loss: 0.8313 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 250 || Loss: 0.7686 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 260 || Loss: 0.9106 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 270 || Loss: 0.7665 || 10iter. || 本イテレーションの正解率：1.0\n",
            "Epoch 1/4 | train |  Loss: 1.1607 Acc: 0.7209\n",
            "Epoch 1/4 |  val  |  Loss: 0.8096 Acc: 0.8713\n",
            "イテレーション 10 || Loss: 0.8020 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 20 || Loss: 0.7100 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 30 || Loss: 0.6208 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 40 || Loss: 1.0275 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 50 || Loss: 0.6770 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 60 || Loss: 0.6304 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 70 || Loss: 0.7536 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 80 || Loss: 0.6910 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 90 || Loss: 0.6960 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 100 || Loss: 0.7346 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 110 || Loss: 1.1538 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 120 || Loss: 0.9223 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 130 || Loss: 1.0417 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 140 || Loss: 0.8091 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 150 || Loss: 0.9330 || 10iter. || 本イテレーションの正解率：0.8125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zuZC_0yaOs"
      },
      "source": [
        "## テストデータでの性能を確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMNf7kVdItkl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxKZzijT5PG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7874e80-4134-445b-a11c-e04d2ecf89df"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForLivedoorに入力\n",
        "        outputs = net_trained(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 93/93 [00:54<00:00,  1.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "テストデータ1475個での正解率：0.9227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvVlrl7y45g"
      },
      "source": [
        "https://yoheikikuta.github.io/bert-japanese/\n",
        "\n",
        "https://github.com/yoheikikuta/bert-japanese\n",
        "\n",
        "の「BERT with SentencePiece for Japanese text.」\n",
        "\n",
        "では、入力テキストにタイトルを含めていますが、今回はタイトルは除いています。\n",
        "\n",
        "同様にタイトルを抜いている、[BERTを用いた日本語文書分類タスクの学習・ハイパーパラメータチューニングの実践例](https://medium.com/karakuri/bert%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E6%97%A5%E6%9C%AC%E8%AA%9E%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AE%E5%AD%A6%E7%BF%92-%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E5%AE%9F%E8%B7%B5%E4%BE%8B-2fa5e4299b16)でも、正解率が92%ちょっととなっており、ほぼ同じ正解率が得られました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y37pHqN2YD0"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}